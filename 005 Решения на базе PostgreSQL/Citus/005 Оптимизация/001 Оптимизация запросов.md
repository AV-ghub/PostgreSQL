## [Шардирование таблиц](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#table-distribution-and-shards)
В качестве поля для шардирования выбираем то что либо
* участвует в объединениях
* используется в фильтрах

Это позволяет исключить неиспользуемые шарды. 
Кроме того, Citus делегитрует вычисления на рабочие узлы, распределяя нагрузку и сокращая сетевой трафик.

## [Оптимизация рабочих узлов](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#postgresql-tuning)
Узлы представляют собой ничто иное как просто экземпляры PG. Координатор распределяет запросы для параллельного прцессинга по узлам, а узлы в свою очередь просто работают в качестве отдельного экземпляра.
Сооотв к ним относится все что касается оптимизации PG.

### [Техника оптимизации](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#postgresql-tuning)
Пускаем запрос на основном узле
```
EXPLAIN VERBOSE
 SELECT...
```
Собираем план. в котором будет присутствовать контекстный запрос со всеми контекстными объектами узла.
Далее идем с этим запросом на узел и оптимизируем его. Далее пропагандируем оптимизацию на все узлы.

Чтобы увидеть всю картину по всем узлам, ставим флаг
```
SET citus.explain_all_tasks = 1;
```
После чего получаем разворотку EXPLAIN по всем узлам.
реальное положение узнаем при помощи EXPLAIN ANALYZE, которая сортирует планы по убыванию времени выполнения. а без разворотки (citus.explain_all_tasks = 0) показывает самый длительный план.

> Разница в выполнении может быть из-за железа, конфигурации или локальных перекосов данных.

### [Масштабирование производительности](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#scaling-out-performance)
В идеале правильное шардирование должно привести к возможности выполнения запросов на отдельной шарде **в пределах** имеющейся там **оперативнгой памяти**. При нехватке ее под текущие задачи рекомендуется испотльзовать **хотя бы SSD**.
При распределении точечных запросов на шарды, речь, как правило, идет про **рандомное чтение**, и HDD здесь дает наихудшие показатели.

Кроме того, если речь идет про интенсив в плане вычислений, следует рассмотреть вопрос наращивания ядер **CPU**.

Следующим моментом является тема параллелизма. Для достижения его статистически приемлемого уровня, необходимо на каждой рабочей ноде создавать **количество шард**, не меньшее **количества ядер CPU**. И принимать во внимание, что часть шард при запросах будет игнорироваться. Здесь тонкий момент средне статистического попадания в оптимальное значение.

## [Распределенный тюнинг](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#distributed-query-performance-tuning)
Первоначально необхлодимо пустить запрос на координаторе и соотв его часть на рабочем узле (установив **\timing**). Это даст представление о проблемах на собственно распределенном запросе.
Далее тюнинг рассматриваем в двух частях - основной и расширенный.

### [Основной](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#general)
С учетом распределения данных по шардам, увеличение потоков вставки в целом сильно повышает общий уровень вставки (также видимо в связи с неактуальностью PK на распределенной схеме.).

Особое внимание следует уделять конструкции подзапросов и CTE.

> По всей видимости это самое больное место распределенных вычислений.

Необходимо всячески избегать [Subquery/CTE Push-Pull Execution](https://docs.citusdata.com/en/v12.0/develop/reference_processing.html#subquery-cte-push-pull-execution)
Для этих целей стараться колоцировать данные, используемые в подзапросах, чтобы они могли четко делегироваться для выполнения на узлах, имея полный набор необходимых данных в пределах узла.

В случае, если это невозможно, стараться минимизировать как возвращаемый из подзапроса на координатор массив данных, так и количество рабочих узлов, на которые будет пропагандироваться в итоге результат подзапроса.

В случае колоцирования данных CTE, рекомендуется использовать технику его материализации [1](https://postgrespro.ru/docs/postgresql/16/queries-with#QUERIES-WITH-CTE-MATERIALIZATION) [2](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CTE-MATERIALIZATION). Когда запрос CTE встраивается в родительский (использующий результаты CTE) запрос и т.о. может выполнять фильтрованную родительским запросом часть на соотв рабочем узле (разумеется, как указывалось выше. при условии колоцирования данных подзапроса).





















