## [Шардирование таблиц](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#table-distribution-and-shards)
В качестве поля для шардирования выбираем то что либо
* участвует в объединениях
* используется в фильтрах

Это позволяет исключить неиспользуемые шарды. 
Кроме того, Citus делегитрует вычисления на рабочие узлы, распределяя нагрузку и сокращая сетевой трафик.

## [Оптимизация рабочих узлов](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#postgresql-tuning)
Узлы представляют собой ничто иное как просто экземпляры PG. Координатор распределяет запросы для параллельного процессинга по узлам, а узлы в свою очередь просто работают в качестве отдельного экземпляра.
Сооотв к ним относится все что касается оптимизации PG.

### [Техника оптимизации](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#postgresql-tuning)
Пускаем запрос на основном узле
```
EXPLAIN VERBOSE
 SELECT...
```
Собираем план. в котором будет присутствовать контекстный запрос со всеми контекстными объектами узла.
Далее идем с этим запросом на узел и оптимизируем его. Далее пропагандируем оптимизацию на все узлы.

Чтобы увидеть всю картину по всем узлам, ставим флаг
```
SET citus.explain_all_tasks = 1;
```
После чего получаем разворотку EXPLAIN по всем узлам.
реальное положение узнаем при помощи EXPLAIN ANALYZE, которая сортирует планы по убыванию времени выполнения. а без разворотки (citus.explain_all_tasks = 0) показывает самый длительный план.

> Разница в выполнении может быть из-за железа, конфигурации или локальных перекосов данных.

### [Масштабирование производительности](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#scaling-out-performance)
В идеале правильное шардирование должно привести к возможности выполнения запросов на отдельной шарде **в пределах** имеющейся там **оперативнгой памяти**. При нехватке ее под текущие задачи рекомендуется использовать **хотя бы SSD**.
При распределении точечных запросов на шарды, речь, как правило, идет про **рандомное чтение**, и HDD здесь дает наихудшие показатели.

Кроме того, если речь идет про интенсив в плане вычислений, следует рассмотреть вопрос наращивания ядер **CPU**.

Следующим моментом является тема параллелизма. Для достижения его статистически приемлемого уровня, необходимо на каждой рабочей ноде создавать **количество шард**, не меньшее **количества ядер CPU**. И принимать во внимание, что часть шард при запросах будет игнорироваться. Здесь тонкий момент средне-статистического попадания в оптимальное значение.

## [Распределенный тюнинг](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#distributed-query-performance-tuning)
Первоначально необхлодимо пустить запрос на координаторе и соотв его часть на рабочем узле (установив **\timing**). Это даст представление о проблемах на собственно распределенном запросе.
Далее тюнинг рассматриваем в двух частях - основной и расширенный.

### [Основной](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#general)
С учетом распределения данных по шардам, увеличение потоков вставки в целом сильно повышает общий уровень вставки (также видимо в связи с неактуальностью PK на распределенной схеме).

Особое внимание следует уделять конструкции подзапросов и CTE.

> По всей видимости это самое больное место распределенных вычислений.

Необходимо всячески избегать [Subquery/CTE Push-Pull Execution](https://docs.citusdata.com/en/v12.0/develop/reference_processing.html#subquery-cte-push-pull-execution)
Для этих целей стараться колоцировать данные, используемые в подзапросах, чтобы они могли четко делегироваться для выполнения на узлах, имея полный набор необходимых данных в пределах узла.

В случае, если это невозможно, стараться минимизировать как возвращаемый из подзапроса на координатор массив данных, так и количество рабочих узлов, на которые будет пропагандироваться в итоге результат подзапроса.

В случае колоцирования данных CTE, рекомендуется использовать технику его материализации ([1](https://postgrespro.ru/docs/postgresql/16/queries-with#QUERIES-WITH-CTE-MATERIALIZATION) [2](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CTE-MATERIALIZATION)). Когда запрос CTE встраивается в родительский (использующий результаты CTE) запрос и т.о. может выполнять фильтрованную родительским запросом часть на соотв рабочем узле (разумеется, как указывалось выше, при условии колоцирования данных подзапроса).

### [Расширенный](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#advanced)
#### [Пул соединений](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#connection-management)
* [citus.max_adaptive_executor_pool_size (integer)](https://docs.citusdata.com/en/v12.0/develop/api_guc.html#max-adaptive-executor-pool-size) очень похож по сути на MAXDOP. Отсюда рекомендации по его применению - 1-2 для коротких запросов, до 16 для аналитических.
* Обратно [citus.executor_slow_start_interval (integer)](https://docs.citusdata.com/en/v12.0/develop/api_guc.html#executor-slow-start-interval) взвинчиваем для коротких запросов, чтобы неповадно было открывать новые соединения и увеличивать параллелизм, и наоборот, умеьшаем для заведомо тяжелых запросов, чтобы пул параллелился сразу.
* [citus.max_cached_conns_per_worker (integer)](https://docs.citusdata.com/en/v12.0/develop/api_guc.html#max-cached-conns-per-worker) сооотв имеет смысл держать для бомбардирующих запросов. Аналитике можно дать развернуться на низком citus.executor_slow_start_interval (integer), а штучным OLTP кэш не нужен.
* [citus.max_shared_pool_size (integer)](https://docs.citusdata.com/en/v12.0/develop/api_guc.html#citus-max-shared-pool-size-integer) рекомендуется ставить значительно выше max_connections limit, поскольку есть ряд операций, которые не ориентируются на параметры лимита соединений. Кроме того, есть процессы репартиционирования, которые тоже никого не спрашивая забирают себе соединения из пула. Таким образом, при низком значении мы можем начать штабелями ронять запросы на сильной нагрузке.

#### [Политика распределения задач по раблочим узлам](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#task-assignment-policy)
В зависимости от целей и существующей конфигурации есть три варианта распределения.
##### Greedy
Равномерное распределение по всем узлам. Подходит для большинства задач.
##### Round-robin
Применяется, когда количество шард относительно мало в сравнении с количеством рабочих узлов.
Позволяет лучше утилизировать ресурсы кластера.
##### First-replica
Задачи узлам назначаются в соответствии с логическим порядком использования шард, и соотв их расположения на узлах.
Это гарантирует нахождение в памяти на всех задействованных узлах горячих данных, используемых в контексте выполняемого цельного распределенного запроса.

#### [Формат обмена промежуточными данными](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#intermediate-data-transfer-format)
По умолчанию Citus передает данные между узлами в текстовом формате. Сериализация и десериализация в этом случае достаточно накладная.
На версиях **PG выше 13** можно включить бинарный формат сериализации, что значительно дешевле.
Включается он парметром конфигурации [citus.binary_worker_copy_format (boolean)](https://docs.citusdata.com/en/v12.0/develop/api_guc.html#binary-worker-copy-format)

#### [Бинарный протокол](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#binary-protocol)
То же самое что выше только в плане передачи итоговых данных на координатор. Особенно касается больших результатов и некоторых искусственных типов данных (бинарных по сути).
Включается
```
citus.enable_binary_protocol to true
```
По умолчанию для **PG выше 14**. Для 14 и ниже соотв не работает.

### [Масштабирование вставки данных](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#scaling-out-data-ingestion)

#### [Real-time вставки и обновления](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#real-time-insert-and-updates)
Citus позволят выполнять непосредственно на координаторе
```
INSERT, INSERT .. ON CONFLICT, UPDATE, and DELETE
```
Все изменения становятся доступны моментально.
В момент вставки Citus на координаторе смотрит метаданные, определяет рабочий узел, исходя из прописанного в метадате расположения требуемой шарды. коннектится и делает конкурентные (параллельные) вставки, достигая этим высокой пропускной способности на вставку.

##### [Вставка) вставки](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#insert-throughput)
> [Benchmark Setup with Citus and pgbench](https://docs.citusdata.com/en/v12.0/extra/write_throughput_benchmark.html#benchmark-setup-with-citus-and-pgbench)

Базовый кластер с 2 ядрами и 7Гб на координаторе + 2 рабочих одноядерных узла с 15Гб позволяет достичь 9к TPC (775 млн вставок в сутки).
Кластер с 8 ядрами и 30Гб на координаторе + 4 рабочих одноядерных узла с 15Гб позволяет достичь 30к TPC (2+ млрд в сутки).

##### [Обновление](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#update-throughput)
На обновлениях при конфигурациях выше удается достичь 10к и 24к соотв.
Обновления создают неактуальные версии строк и требуют соотв VACUUM. При этом последний при работе создает минимальные накладные расходы, т.к. работает в параллель на свободных узлах.

##### [Контрольный лист](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#insert-and-update-throughput-checklist)
Грамотно настроенный кластер обеспечивает пропускную способность 10-50к вставок/сек.
Для достижения показателей необходимо принимать во внимание следующее.
* Большое внимание следует уделять сетевым задержкам, они во многом определяют пропускную способность
* Вставки необходимо делать параллельными потоками. бщая пропускная способность фактически прямо пропорциональна количеству входных потоков
* Стандартно контролируем, что рабочие узлы не испытывают проблем по CPU и дискам
* Большое внимание уделяем CPU на координаторе, это вместе с первым пунктом самая критичная точка для общей производительности
* Необходимо обеспечить кэширование существующих соединений между вставками, постоянное пересоздание сильно влияет на итоговый результат
* По архитектуре следует принимать во внимание, что критичен размер полей. Чем он ни меньше, тем эффективнее результат. Проблема с JSON, BLOB & XML. Вариант разбивки широких таблиц на объектно значимые с целевой вставкой только нужных данных.

##### [Замечания по задержкам](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#insert-and-update-latency)
Пример ниже наглядно демонстрирует накладные расходоы на создание подключений (соединения удерживаются в рамках сессии).
```
-- Set up a distributed table that keeps account history information
CREATE TABLE pgbench_history (tid int, bid int, aid int, delta int, mtime timestamp);
SELECT create_distributed_table('pgbench_history', 'aid');

-- Enable timing to see reponse times
\timing on

-- First INSERT requires connection set-up, second will be faster
INSERT INTO pgbench_history VALUES (10, 1, 10000, -5000, CURRENT_TIMESTAMP); -- Time: 10.314 ms
INSERT INTO pgbench_history VALUES (10, 1, 22000, 5000, CURRENT_TIMESTAMP); -- Time: 3.132 ms
```
#### [Загрузка временных данных](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#staging-data-temporarily)
При загрузке оперативных данных для вычислений следует пользоваться [нелогируемыми таблицами](https://www.postgresql.org/docs/current/static/sql-createtable.html#SQL-CREATETABLE-UNLOGGED).
```
-- example unlogged table
CREATE UNLOGGED TABLE unlogged_table (
  key text,
  value text
);

-- its shards will be unlogged as well when
-- the table is distributed
SELECT create_distributed_table('unlogged_table', 'key');

-- ready to load data
```

#### [Потоковая загрузка](https://docs.citusdata.com/en/v12.0/performance/performance_tuning.html#bulk-copy-250k-2m-s)
[Потоковая загрузка](http://www.postgresql.org/docs/current/static/sql-copy.html) позволяет достичь более высоких показателей вставки.
Загрузку можно осуществлять как из файла, так и непосредственно из входного потока.
```
COPY pgbench_history FROM STDIN WITH (FORMAT CSV);
```
Psql фактически формирует предыдущую команду перед обращением к файлу.
```
psql -c "\COPY pgbench_history FROM 'pgbench_history-2016-03-04.csv' (FORMAT CSV)"
```
Мощь потоковой вставки заключается в том, что она максимально распараллеливает входной поток. Особенно это сказывается при наличии множества или тяжелых индексов на целевой таблице. Скорость вставки при таком вариатне использования может достигать 250к-2млн вставок/сек. Отдельный блог на эту тему [тут](https://www.citusdata.com/blog/2016/06/15/copy-postgresql-distributed-tables?_gl=1*9qzqhf*_ga*MjAzMjEzMDkxNi4xNzA1MzAxODI1*_ga_DS5S1RKEB7*MTcwNjI2NTU4Ni44LjEuMTcwNjI2NTYwMS4wLjAuMA..).

##### Рекомендации по массовой загрузке
* Рекомедуемый размер пакета 50к-100к. Идеально грузить множество файлов с этим размером на каждый.
* Следует предварительно последовательно протестировать степень входного параллелизма в 2,4,8,16
* Большую роль играют CPU оптимизации координатора
* Рекомендуется использовать относительно небольшое количество шард - 32, возможно 64, не больше
* Тестирование следует проводить на больших промежутках в 2,4,8,24ч. Чем ни больше, тем будет более точное представление о поведении на проде.





